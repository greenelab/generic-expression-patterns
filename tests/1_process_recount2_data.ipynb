{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process recount2 data\n",
    "This notebook does the following:\n",
    "\n",
    "1. Selects template experiment\n",
    "2. Downloads subset of recount2 data, including the template experiment (subset of random experiments + 1 template experiment)\n",
    "3. Train VAE on subset of recount2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/alexandra/anaconda3/envs/generic_expression_new/lib/python3.7/site-packages/ponyo/helper_vae.py:21: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/alexandra/anaconda3/envs/generic_expression_new/lib/python3.7/site-packages/ponyo/helper_vae.py:25: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/alexandra/anaconda3/envs/generic_expression_new/lib/python3.7/site-packages/ponyo/helper_vae.py:25: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/alexandra/anaconda3/envs/generic_expression_new/lib/python3.7/site-packages/matplotlib/__init__.py:886: MatplotlibDeprecationWarning: \n",
      "examples.directory is deprecated; in the future, examples will be found relative to the 'datapath' directory.\n",
      "  \"found relative to the 'datapath' directory.\".format(key))\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext rpy2.ipython\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ponyo import utils, train_vae_modules\n",
    "from generic_expression_patterns_modules import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/alexandra/Documents/Repos/generic-expression-patterns/generic_expression_patterns_modules/process.py:57: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set seeds to get reproducible VAE trained models\n",
    "process.set_all_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.abspath(os.path.join(os.getcwd(), \"../\"))\n",
    "\n",
    "# Read in config variables\n",
    "config_filename = os.path.abspath(os.path.join(base_dir, \"configs\", \"config_test.tsv\"))\n",
    "\n",
    "params = utils.read_config(config_filename)\n",
    "\n",
    "local_dir = params[\"local_dir\"]\n",
    "dataset_name = params[\"dataset_name\"]\n",
    "\n",
    "# File that contains gene ranks identified by Crow et. al.\n",
    "DE_prior_filename = params[\"reference_gene_filename\"]\n",
    "\n",
    "# Template experiment ID\n",
    "project_id = params[\"project_id\"]\n",
    "\n",
    "# Output file: pickled list of shared genes(generated during gene ID mapping)\n",
    "shared_genes_filename = params[\"shared_genes_filename\"]\n",
    "\n",
    "# Output files of recount2 template experiment data\n",
    "raw_template_filename = params[\"raw_template_filename\"]\n",
    "mapped_template_filename = params[\"mapped_template_filename\"]\n",
    "\n",
    "# Output files of recount2 compendium data\n",
    "raw_compendium_filename = params[\"raw_compendium_filename\"]\n",
    "mapped_compendium_filename = params[\"mapped_compendium_filename\"]\n",
    "normalized_compendium_filename = params[\"normalized_compendium_filename\"]\n",
    "\n",
    "# Output file: pickled scaler (generated during compendium normalization)\n",
    "scaler_filename = params[\"scaler_filename\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure this directory already exists\n",
    "os.makedirs(local_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-downloaded and saved the template data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Renaming gene ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File mapping ensembl ids to hgnc symbols\n",
    "gene_id_filename = os.path.join(\n",
    "    base_dir, dataset_name, \"data\", \"metadata\", \"ensembl_hgnc_mapping.tsv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i raw_template_filename -i gene_id_filename -i base_dir\n",
    "\n",
    "# Get mapping between ensembl gene ids (ours) to HGNC gene symbols (published)\n",
    "# Input: raw_template_filename, output: gene_id_filename\n",
    "\n",
    "source(paste0(base_dir, '/generic_expression_patterns_modules/process_names.R'))\n",
    "\n",
    "# Note: This mapping file from ensembl ids to hgnc symbols is based on the library(\"biomaRt\")\n",
    "# that gets updated. In order to get the most up-to-date version, you can delete the\n",
    "# ensembl_hgnc_mapping file to re-run the script that generates this mapping.\n",
    "\n",
    "if (file.exists(gene_id_filename) == FALSE) {\n",
    "    get_ensembl_symbol_mapping(raw_template_filename, gene_id_filename)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: processing template data\n",
    "This step will map the ensembl gene IDs in raw template data file to hgnc gene symbols, and delete certain columns (genes) and rows (samples).\n",
    "\n",
    "Output files generated in this step:\n",
    "- `shared_genes_filename`: pickled list of shared genes (created only if it doesn't exist yet)\n",
    "- `mapped_template_filename`: template data with column names mapped to hgnc gene symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_mapping = {\n",
    "    \"ENSG00000187510.7\": \"PLEKHG7\",\n",
    "    \"ENSG00000230417.11\": \"LINC00595\",\n",
    "    \"ENSG00000276085.1\": \"CCL3L1\",\n",
    "    \"ENSG00000255374.3\": \"TAS2R45\",\n",
    "}\n",
    "\n",
    "process.map_recount2_data(\n",
    "    raw_template_filename,\n",
    "    gene_id_filename,\n",
    "    manual_mapping,\n",
    "    DE_prior_filename,\n",
    "    shared_genes_filename,\n",
    "    mapped_template_filename,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Processing compendium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: dataset contains 92 samples and 17788 genes\n"
     ]
    }
   ],
   "source": [
    "process.process_raw_compendium_recount2(\n",
    "    raw_compendium_filename,\n",
    "    gene_id_filename,\n",
    "    manual_mapping,\n",
    "    DE_prior_filename,\n",
    "    shared_genes_filename,\n",
    "    mapped_compendium_filename,\n",
    "    normalized_compendium_filename,\n",
    "    scaler_filename,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of genes is equal between the compendium and the template\n",
    "compendium_data = pd.read_csv(\n",
    "    normalized_compendium_filename, sep=\"\\t\", index_col=0, header=0\n",
    ")\n",
    "\n",
    "template_data = pd.read_csv(mapped_template_filename, header=0, sep=\"\\t\", index_col=0)\n",
    "assert compendium_data.shape[1] == template_data.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train: VAE training and reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VAE directories\n",
    "output_dirs = [\n",
    "    os.path.join(base_dir, dataset_name, \"models\"),\n",
    "    os.path.join(base_dir, dataset_name, \"logs\"),\n",
    "]\n",
    "NN_architecture = params[\"NN_architecture\"]\n",
    "\n",
    "for each_dir in output_dirs:\n",
    "    new_dir = os.path.join(each_dir, NN_architecture)\n",
    "    os.makedirs(new_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dataset contains 92 samples and 17788 genes\n",
      "WARNING:tensorflow:From /home/alexandra/anaconda3/envs/generic_expression_new/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "tracking <tf.Variable 'Variable:0' shape=() dtype=float32> beta\n",
      "WARNING:tensorflow:From /home/alexandra/anaconda3/envs/generic_expression_new/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexandra/anaconda3/envs/generic_expression_new/lib/python3.7/site-packages/keras/engine/training_utils.py:819: UserWarning: Output custom_variational_layer_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to custom_variational_layer_1.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/alexandra/anaconda3/envs/generic_expression_new/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 83 samples, validate on 9 samples\n",
      "Epoch 1/5\n",
      "83/83 [==============================] - 27s 325ms/step - loss: 9298.6198 - kl_loss: 15.2871 - recons_loss: 9298.6201 - val_loss: 149359.5625 - val_kl_loss: 68299.2344 - val_recons_loss: 149359.5625\n",
      "Epoch 2/5\n",
      "83/83 [==============================] - 27s 322ms/step - loss: 7997.1775 - kl_loss: 14.9666 - recons_loss: 7997.1777 - val_loss: 25085.4284 - val_kl_loss: 1775.7587 - val_recons_loss: 25085.4297\n",
      "Epoch 3/5\n",
      "83/83 [==============================] - 27s 321ms/step - loss: 7876.3152 - kl_loss: 15.7234 - recons_loss: 7876.3154 - val_loss: 15018.1660 - val_kl_loss: 434.7598 - val_recons_loss: 15018.1670\n",
      "Epoch 4/5\n",
      "83/83 [==============================] - 27s 323ms/step - loss: 7889.4818 - kl_loss: 15.0087 - recons_loss: 7889.4829 - val_loss: 10852.7154 - val_kl_loss: 254.6292 - val_recons_loss: 10852.7148\n",
      "Epoch 5/5\n",
      "83/83 [==============================] - 27s 325ms/step - loss: 7849.6292 - kl_loss: 15.2351 - recons_loss: 7849.6294 - val_loss: 10629.0273 - val_kl_loss: 162.7097 - val_recons_loss: 10629.0273\n"
     ]
    }
   ],
   "source": [
    "# Train VAE on new compendium data\n",
    "train_vae_modules.train_vae(config_filename, normalized_compendium_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reproducibility\n",
    "expected_log = \"data/test_vae_logs.tsv\"\n",
    "actual_log = \"logs/NN_2500_30/tybalt_2layer_30latent_stats.tsv\"\n",
    "assert pd.read_csv(actual_log, sep=\"\\t\")[\"val_loss\"].values[-1] < 15000, pd.read_csv(\n",
    "    actual_log, sep=\"\\t\"\n",
    ")[\"val_loss\"].values[-1]"
   ]
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1592246126078,
   "trusted": true
  },
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python [conda env:generic_expression_new] *",
   "language": "python",
   "name": "conda-env-generic_expression_new-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
